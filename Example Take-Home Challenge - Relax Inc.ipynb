{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to predict if a user will be a 'adopted user' of a software program developed by Relax Inc., who \"makes productivity and project management software that's popular with both individuals and teams\".\n",
    "\n",
    "Adopted User = \"a user who has logged into the product on three separate days in at least one seven day period\"\n",
    "\n",
    "The resulting models generated a predictive accuracy of ~98% or ~86% depending on what variables was included in the input dataset. If the total number of times a user logged into the program was included, then the model had a predictive accuracy of ~98%. However, if the that variable was not included, then the model had a accuracy of ~86%. See PDF write-up for conclusions.\n",
    "\n",
    "The methods used in this are Data Manipulation, Data Wrangling, and Machine Learning with Random Forest using Scikit-Learn/Sklearn. Because of the high performance of the random forest models, no other models were created.\n",
    "\n",
    "Required Files Before Running:\n",
    "- takehome_users.csv\n",
    "- takehome_user_engagement.csv\n",
    "\n",
    "These must be in the same folder as this script to run correctly. Else, change the filepath in the next cell to direct to these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the two data sets:\n",
    "## For the takehome_users_modified.csv file, loading with the the latin-1 codec was necessary as the standard utf-8 was \n",
    "## unable to load without errors.\n",
    "\n",
    "users_df = pd.read_csv('takehome_users.csv', infer_datetime_format= True, encoding= 'latin-1')\n",
    "logins_df = pd.read_csv('takehome_user_engagement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logins_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the label 'adopted user' according to its definition:\n",
    "# \"a user who has logged into the product on three separate days in at least one seven day period\"\n",
    "# I will be interpreting the definition as a user who has logged in for 3 days within a 7 day period.\n",
    "# A user labeled as an adopted user will be assigned a value of 1 and non-adopted users will be assigned a value of 0\n",
    "\n",
    "# Data Manipulation - Changing all time values into time data types:\n",
    "logins_df['time_stamp'] = pd.to_datetime(logins_df['time_stamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Creating a new DataFrame for the total number of logins and the label of the user:\n",
    "label_df = pd.DataFrame({'USER_ID': range(1, 12001), 'TOTAL_LOGINS':np.NaN, 'LABEL':np.NaN})\n",
    "\n",
    "# Filling in label_df with logins & labels:\n",
    "for index, login in label_df.iterrows():\n",
    "    temp_df = pd.DataFrame() # This will 'clean' the declaration from previous iterations/error prevention\n",
    "    login_count = 0\n",
    "    week_difference = datetime.timedelta(days=7)\n",
    "    label = np.NaN\n",
    "    \n",
    "    temp_df = logins_df[logins_df.user_id == login.USER_ID].copy()\n",
    "\n",
    "    login_count = len(temp_df)\n",
    "    label_df.loc[index, 'TOTAL_LOGINS'] = login_count\n",
    "\n",
    "    if login_count < 3:\n",
    "        label = 0\n",
    "\n",
    "    else:\n",
    "        for temp_index, temp_row in temp_df.iterrows():\n",
    "            start_date = temp_row.time_stamp.date()\n",
    "            end_date = start_date + week_difference\n",
    "\n",
    "            second_login = np.NaN\n",
    "            third_login = np.NaN\n",
    "\n",
    "            try: # This is necessary to prevent indexing errors when accessing indexes in the next two rows\n",
    "                second_login = temp_df.loc[temp_index + 1].time_stamp.date() # This indexing works because the rows are \n",
    "                third_login = temp_df.loc[temp_index + 2].time_stamp.date() # already ordered by user ID, then date\n",
    "\n",
    "                if second_login > start_date and second_login < end_date:\n",
    "                    if third_login > second_login and third_login <= end_date:\n",
    "                        label = 1\n",
    "                        break\n",
    "\n",
    "            except Exception:\n",
    "                label = 0 # This means that of all the logins, there were not any three logins made within a week\n",
    "                break\n",
    "\n",
    "    label_df.loc[index, 'LABEL'] = label\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is to check to see if there are any np.NaN values left from the initialization of label_df:\n",
    "label_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final DataFrame:\n",
    "final_df = users_df.copy()\n",
    "\n",
    "## Merging label_df:\n",
    "final_df = final_df.merge(label_df, how= 'left', left_on= 'object_id', right_on= 'USER_ID')\n",
    "\n",
    "## Removing columns object_id, creation_time, name, email, last_session_creation_time, org_id, and USER_ID because they are  \n",
    "## not relevant to the definition of what is considered to be an adopted user.\n",
    "final_df = final_df.drop(['object_id', \n",
    "                          'creation_time', \n",
    "                          'name', \n",
    "                          'email', \n",
    "                          'last_session_creation_time', \n",
    "                          'org_id', \n",
    "                          'USER_ID'], \n",
    "                         axis= 1)\n",
    "\n",
    "## Changing creation_source & invited_by_user_id to categorical values:\n",
    "for index, row in final_df.iterrows():\n",
    "    \n",
    "    ###### creation_source categorical assignments:\n",
    "    ### PERSONAL_PROJECTS = 0\n",
    "    ### GUEST_INVITE = 1\n",
    "    ### ORG_INVITE = 2\n",
    "    ### SIGNUP = 3\n",
    "    ### SIGNUP_GOOGLE_AUTH = 4\n",
    "    \n",
    "    temp_dict = {'PERSONAL_PROJECTS': 0, 'GUEST_INVITE': 1, 'ORG_INVITE': 2, 'SIGNUP': 3, 'SIGNUP_GOOGLE_AUTH': 4}\n",
    "    \n",
    "    final_df.loc[index, 'creation_source'] = temp_dict[row.creation_source]\n",
    "    \n",
    "    ### If the user was invited by another, then it will be assigned a 1. If the value is a np.NaN, then it will be assigned \n",
    "    ### a 0.\n",
    "    category = np.NaN\n",
    "    \n",
    "    if np.isnan(row.invited_by_user_id) == True:\n",
    "        category = 0\n",
    "    \n",
    "    elif row.invited_by_user_id > 0:\n",
    "        category = 1\n",
    "        \n",
    "    final_df.loc[index, 'invited_by_user_id'] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing final_df for machine learning:\n",
    "##### There will be two different types of data to be feed into the machine learning algorithms; one type will have the \n",
    "##### total number of logins a user has made (all variables with \"with_total\") and the other type will not have this \n",
    "##### information (all variables with \"no_total\").\n",
    "\n",
    "# Splitting the label from the variables:\n",
    "variables_with_total = final_df.iloc[:, :5].copy()\n",
    "labels_with_total = final_df.iloc[:, -1].astype('int32') # .copy() is already default for .astype()\n",
    "\n",
    "variables_no_total = final_df.iloc[:, :4].copy()\n",
    "labels_no_total = final_df.iloc[:, -1].astype('int32') # .copy() is already default for .astype()\n",
    "\n",
    "# Splitting the dataset into train and test sets:\n",
    "## Note: Shuffling is not necessary since the original user_df is in order of the \"object_id\"/user ID, which has been removed\n",
    "## and the assignment of the user ID to the user was not done in order of when the account was created. However, shuffle is\n",
    "## the default for the train_test_split() function and was keep True in this script.\n",
    "X_train_with_total, X_test_with_total, Y_train_with_total, Y_test_with_total = train_test_split(variables_with_total, \n",
    "                                                                                                labels_with_total, \n",
    "                                                                                                test_size=0.25)\n",
    "\n",
    "X_train_no_total, X_test_no_total, Y_train_no_total, Y_test_no_total = train_test_split(variables_no_total, labels_no_total, \n",
    "                                                                                        test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-Learn's Random Forest to find Feature Importance:\n",
    "\n",
    "## Finding the best number of trees to use from the recommended standard of 64-128 trees:\n",
    "temp_df = pd.DataFrame({'TREES': range(64, 128), 'ACCURACY': np.NaN})\n",
    "\n",
    "for temp_index, temp_row in temp_df.iterrows():\n",
    "    trees = int(temp_row.TREES)\n",
    "    random_forest = RandomForestClassifier(n_estimators= trees, max_features= 'sqrt')\n",
    "    random_forest.fit(X_train_with_total, Y_train_with_total)\n",
    "    temp_df.loc[temp_index, 'ACCURACY'] = random_forest.score(X_test_with_total, Y_test_with_total)\n",
    "    \n",
    "###### The average is taken here in the case that there are two or more top scoring trees parameters\n",
    "best_trees = int(np.average(temp_df.TREES[temp_df.ACCURACY == max(temp_df.ACCURACY)]))  \n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators= best_trees, max_features= 'sqrt')\n",
    "random_forest.fit(X_train_with_total, Y_train_with_total)\n",
    "best_accuracy_with_total = random_forest.score(X_test_with_total, Y_test_with_total)\n",
    "feature_importance_with_total = random_forest.feature_importances_\n",
    "feature_importance_df_with_total = pd.DataFrame({'creation_source': feature_importance_with_total[0], \n",
    "                                      'opted_in_to_mailing_list': feature_importance_with_total[1],\n",
    "                                      'enabled_for_marketing_drip': feature_importance_with_total[2], \n",
    "                                      'invited_by_user_id': feature_importance_with_total[3], \n",
    "                                      'TOTAL_LOGINS': feature_importance_with_total[4]}, \n",
    "                                     index=[0])\n",
    "\n",
    "\n",
    "## Repeating process for the dataset without the total number of logins:\n",
    "temp_df = pd.DataFrame({'TREES': range(64, 128), 'ACCURACY': np.NaN})\n",
    "\n",
    "for temp_index, temp_row in temp_df.iterrows():\n",
    "    trees = int(temp_row.TREES)\n",
    "    random_forest = RandomForestClassifier(n_estimators= trees, max_features= 'sqrt')\n",
    "    random_forest.fit(X_train_no_total, Y_train_no_total)\n",
    "    temp_df.loc[temp_index, 'ACCURACY'] = random_forest.score(X_test_no_total, Y_test_no_total)\n",
    "    \n",
    "best_trees = int(np.average(temp_df.TREES[temp_df.ACCURACY == max(temp_df.ACCURACY)]))  \n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators= best_trees, max_features= 'sqrt')\n",
    "random_forest.fit(X_train_no_total, Y_train_no_total)\n",
    "best_accuracy_no_total = random_forest.score(X_test_no_total, Y_test_no_total)\n",
    "feature_importance_no_total = random_forest.feature_importances_\n",
    "feature_importance_df_no_total = pd.DataFrame({'creation_source': feature_importance_no_total[0], \n",
    "                                      'opted_in_to_mailing_list': feature_importance_no_total[1],\n",
    "                                      'enabled_for_marketing_drip': feature_importance_no_total[2], \n",
    "                                      'invited_by_user_id': feature_importance_no_total[3]},\n",
    "                                     index=[0])\n",
    "\n",
    "feature_importance_df = pd.concat([feature_importance_df_with_total, feature_importance_df_no_total], ignore_index= True)\n",
    "feature_importance_df.loc[0, 'ACCURACY'] = best_accuracy_with_total\n",
    "feature_importance_df.loc[1, 'ACCURACY'] = best_accuracy_no_total\n",
    "feature_importance_df.index = ['WITH TOTAL', 'WITHOUT TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "###### The total of logins a user has seems to be the extremely domininate factor in the random forest trained with the  \n",
    "###### dataset containing the total number of logins for each user.\n",
    "######## The creation_source would be the most domininate factor in the model trained without the login counts.\n",
    "########### The accuracy value is significantly better for the model trained with the total counts. However, the model \n",
    "########### trained without the total counts show more insight on the user without login information known prior.\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to investigate which creation_source category was most common to users labelled as adopted.\n",
    "examine_df = final_df[final_df.LABEL == 1]\n",
    "print('PERSONAL_PROJECTS: ', len(examine_df[examine_df.creation_source == 0]) / len(examine_df))\n",
    "print('GUEST_INVITE: ', len(examine_df[examine_df.creation_source == 1]) / len(examine_df))\n",
    "print('ORG_INVITE: ', len(examine_df[examine_df.creation_source == 2]) / len(examine_df))\n",
    "print('SIGNUP: ', len(examine_df[examine_df.creation_source == 3]) / len(examine_df))\n",
    "print('SIGNUP_GOOGLE_AUTH: ', len(examine_df[examine_df.creation_source == 4]) / len(examine_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
